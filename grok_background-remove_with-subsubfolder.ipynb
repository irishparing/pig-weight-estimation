{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM model and move to GPU\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"  # Adjust path to your downloaded checkpoint\n",
    "model_type = \"vit_h\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_background(image):\n",
    "    \"\"\"\n",
    "    Removes the background of an image using SAM with a center point prompt on GPU.\n",
    "    \"\"\"\n",
    "    # Convert to RGB for SAM\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    predictor.set_image(image_rgb)  # Preprocess image (should move to GPU internally)\n",
    "\n",
    "    # Use center of image as prompt\n",
    "    height, width = image.shape[:2]\n",
    "    center_point = np.array([[width // 2, height // 2]], dtype=np.float32)  # Keep as NumPy array\n",
    "    point_label = np.array([1], dtype=np.int64)  # Keep as NumPy array\n",
    "\n",
    "    # Ensure prediction runs on GPU and returns tensors\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        masks, scores, _ = predictor.predict(\n",
    "            point_coords=center_point,  # Pass NumPy array\n",
    "            point_labels=point_label,   # Pass NumPy array\n",
    "            multimask_output=False,\n",
    "            return_logits=False\n",
    "        )\n",
    "\n",
    "    # Select best mask\n",
    "    mask_binary = masks[np.argmax(scores)]\n",
    "\n",
    "    # Move to CPU and convert to NumPy\n",
    "    if isinstance(mask_binary, torch.Tensor):\n",
    "        mask_binary = mask_binary.cpu().numpy()\n",
    "    mask_binary = (mask_binary * 255).astype(np.uint8)\n",
    "\n",
    "    # Create RGBA image with transparent background\n",
    "    result = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
    "    result[:, :, 3] = mask_binary\n",
    "\n",
    "    return result, mask_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_to_depth(depth_image, mask):\n",
    "    if mask.dtype != np.uint8:\n",
    "        mask = mask.astype(np.uint8)\n",
    "    if len(depth_image.shape) == 3:\n",
    "        if mask.shape != depth_image.shape[:2]:\n",
    "            mask = cv2.resize(mask, (depth_image.shape[1], depth_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        result = depth_image.copy()\n",
    "        for channel in range(result.shape[2]):\n",
    "            result[:, :, channel] = cv2.bitwise_and(result[:, :, channel], result[:, :, channel], mask=mask)\n",
    "        result_bgra = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "        result_bgra[:, :, 3] = mask\n",
    "    else:\n",
    "        if mask.shape != depth_image.shape:\n",
    "            mask = cv2.resize(mask, (depth_image.shape[1], depth_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        result = cv2.bitwise_and(depth_image, depth_image, mask=mask)\n",
    "        result_bgra = cv2.cvtColor(result, cv2.COLOR_GRAY2BGRA)\n",
    "        result_bgra[:, :, 3] = mask\n",
    "    return result_bgra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subfolder(subdir, output_subdir):\n",
    "    output_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Modified pattern to match files with incremental numbers\n",
    "    image_files = sorted(subdir.glob(\"frame_*_rgb_crop_*.png\"))\n",
    "    for rgb_file in tqdm(image_files, desc=f\"Processing {subdir.name}\"):\n",
    "        # Extract frame number and crop number from filename\n",
    "        stem = rgb_file.stem  # e.g., \"frame_0_rgb_crop_1\"\n",
    "        depth_stem = stem.replace(\"_rgb_crop_\", \"_depth_crop_\")\n",
    "        depth_file = rgb_file.with_name(depth_stem + rgb_file.suffix)\n",
    "\n",
    "        if not depth_file.exists():\n",
    "            print(f\"Warning: No corresponding depth image for {rgb_file.name}\")\n",
    "            continue\n",
    "\n",
    "        # Read images\n",
    "        rgb_image = cv2.imread(str(rgb_file))\n",
    "        depth_image = cv2.imread(str(depth_file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # Remove background with SAM\n",
    "        rgb_no_bg, mask_binary = remove_background(rgb_image)\n",
    "        depth_no_bg = apply_mask_to_depth(depth_image, mask_binary)\n",
    "\n",
    "        # Save output images\n",
    "        cv2.imwrite(str(output_subdir / rgb_file.name), rgb_no_bg)\n",
    "        cv2.imwrite(str(output_subdir / depth_file.name), depth_no_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(input_folder, output_folder):\n",
    "    torch.cuda.empty_cache()\n",
    "    input_path = Path(input_folder)\n",
    "    output_path = Path(output_folder)\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        print(\"Input folder does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for subdir in tqdm(list(input_path.glob('*')), desc=\"Processing folders\"):\n",
    "        if subdir.is_dir():\n",
    "            for nested_subdir in subdir.glob('*'):\n",
    "                if nested_subdir.is_dir():\n",
    "                    process_subfolder(nested_subdir, output_path / nested_subdir.relative_to(input_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_175_rgb_crop_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_320_rgb_crop_1.png\n",
      "Warning: No corresponding depth image for frame_321_rgb_crop_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing long: 100%|██████████| 388/388 [21:30<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_163_rgb_crop_2.png\n",
      "Warning: No corresponding depth image for frame_164_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing short: 100%|██████████| 248/248 [13:56<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_163_rgb_crop_1.png\n",
      "Warning: No corresponding depth image for frame_164_rgb_crop_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_175_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_320_rgb_crop_2.png\n",
      "Warning: No corresponding depth image for frame_321_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing twirl: 100%|██████████| 364/364 [20:08<00:00,  3.32s/it]\n",
      "Processing folders:  50%|█████     | 1/2 [55:35<55:35, 3335.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_153_rgb_crop_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_172_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_180_rgb_crop_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_187_rgb_crop_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_80_rgb_crop_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing long: 100%|██████████| 564/564 [32:02<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_160_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_172_rgb_crop_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_180_rgb_crop_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_187_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_201_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_80_rgb_crop_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing short: 100%|██████████| 573/573 [32:25<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_160_rgb_crop_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_355_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_385_rgb_crop_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No corresponding depth image for frame_519_rgb_crop_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing twirl: 100%|██████████| 538/538 [29:54<00:00,  3.33s/it]\n",
      "Processing folders: 100%|██████████| 2/2 [2:29:57<00:00, 4498.70s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_folder = \".\\\\April_05_2025-Batch3\\\\outputs_1-2\"\n",
    "    output_folder = \".\\\\April_05_2025-Batch3\\\\background_remove_output_segregated\"\n",
    "    process_images(input_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
